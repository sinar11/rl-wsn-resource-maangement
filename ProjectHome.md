Resource management is an essential ingredient of a middleware solution for wireless sensor networks (WSN). Resource management includes initial sensor-selection and task allocation as well as runtime adaptation of allocated task/resources. Because of the highly dynamic and resource constrained nature of sensor networks, many proposed middleware solutions have advocated strong need for proactive adaptation of resources, but there are only a few that have actually tried to resolve the issue of enabling resource management for WSN applications. This problem of resource management/adaptation can be described as follows:
> Given application structure, QoS requirements and current system state, what is the best way of allocating tasks to resources so that a given system-wide, application-driven, global optimization parameter is optimized.
In the above, the application structure is in the form of underlying tasks and their interactions. The QoS requirements include such constraints as latency, reliability, etc. while the current state of the system is defined by parameters like mobility, energy availability, and neighbouring nodes. The parameters to be optimized include energy, bandwidth, and network lifetime.

This project presents Distributed Independent Reinforcement Learning (DIRL) based framework to address the issue of dynamic resource adaptation in WSN. The main idea of DIRL is to allow each individual sensor node to self-schedule its tasks and allocate its resources by learning their usefulness (utility) in any given state while honouring application defined constraints and maximizing total amount of reward over time. DIRL is based on Q-learning, a form of model-free reinforcement learning. Q-learning is quite simple - demands minimal computational resources and doesnâ€™t require a model of the environment in order to operate. Hence it is ideal for implementation on resource-constrained sensor nodes. DIRL uses classic exploration and exploitation strategy as used in most RL based approaches to learn utilities of various tasks. DIRL addresses structural credit assignment (propagation of reward spatially across states in order to define notion of similar states) problem by using weighted hamming distance between two states. Here, the application state is represented in the form of system and application variables each carrying an associated weight.
DIRL employs independent learning where each agent applies the learning algorithm in a classic sense (like single agent system) ignoring the presence of other agents. The main advantage of using independent learning in DIRL is that no communication is required for co-ordination between sensor nodes and each node selfishly tries to maximize its own rewards.

COllective INtelligence (COIN) is a macro-learning paradigm that aims to specifically address the problem of designing utility functions for individual agents in order to achieve higher system wide utility. We further extend DIRL, by combining it with the COIN based macro-learning paradigm to steer the system towards global optimization, improving performance with minimal communication overheads